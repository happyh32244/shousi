{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d47eb6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a5f2c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(128,64,512)  # (batch_size, seq_len, embedding_dim)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3330f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "n_heads = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762931ad",
   "metadata": {},
   "source": [
    "#### 多头注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c854109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output--->tensor([[[ 0.0367,  0.1196,  0.0331,  ...,  0.0248, -0.0792, -0.0767],\n",
      "         [ 0.0482,  0.1216,  0.0219,  ...,  0.0257, -0.0672, -0.0462],\n",
      "         [ 0.0415,  0.1056,  0.0198,  ...,  0.0270, -0.0822, -0.0797],\n",
      "         ...,\n",
      "         [ 0.0553,  0.1517,  0.0564,  ..., -0.0017, -0.0722, -0.0655],\n",
      "         [ 0.0440,  0.1149,  0.0349,  ...,  0.0269, -0.0449, -0.0280],\n",
      "         [ 0.0472,  0.1310,  0.0410,  ..., -0.0052, -0.0670, -0.0557]],\n",
      "\n",
      "        [[ 0.0391,  0.0544, -0.0012,  ...,  0.0549, -0.0275, -0.0609],\n",
      "         [ 0.0023,  0.0430,  0.0061,  ...,  0.0462, -0.0289, -0.0527],\n",
      "         [ 0.0221,  0.0493,  0.0106,  ...,  0.0086, -0.0431, -0.0728],\n",
      "         ...,\n",
      "         [ 0.0563,  0.0281, -0.0107,  ...,  0.0307, -0.0307, -0.0553],\n",
      "         [ 0.0227,  0.0617, -0.0195,  ...,  0.0318, -0.0325, -0.0421],\n",
      "         [-0.0034,  0.0391, -0.0054,  ...,  0.0455, -0.0302, -0.0466]],\n",
      "\n",
      "        [[-0.0370,  0.0946, -0.0143,  ...,  0.0410, -0.0995, -0.0281],\n",
      "         [-0.0353,  0.1263,  0.0035,  ...,  0.0091, -0.0981, -0.0504],\n",
      "         [-0.0303,  0.0973, -0.0346,  ...,  0.0246, -0.0848, -0.0453],\n",
      "         ...,\n",
      "         [-0.0231,  0.1156, -0.0265,  ...,  0.0261, -0.0953, -0.0583],\n",
      "         [-0.0422,  0.1194, -0.0560,  ...,  0.0275, -0.1007, -0.0184],\n",
      "         [-0.0406,  0.1440, -0.0293,  ...,  0.0405, -0.0875, -0.0370]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0739,  0.0872, -0.0336,  ..., -0.0693, -0.0941, -0.0350],\n",
      "         [ 0.0806,  0.0976, -0.0320,  ..., -0.0494, -0.0875, -0.0409],\n",
      "         [ 0.0604,  0.1037, -0.0363,  ..., -0.0360, -0.0790, -0.0420],\n",
      "         ...,\n",
      "         [ 0.0880,  0.0720, -0.0388,  ..., -0.0710, -0.1004, -0.0493],\n",
      "         [ 0.0630,  0.0983, -0.0396,  ..., -0.0457, -0.0744, -0.0248],\n",
      "         [ 0.0687,  0.1116, -0.0348,  ..., -0.0190, -0.0966, -0.0297]],\n",
      "\n",
      "        [[ 0.0540, -0.0646, -0.0505,  ...,  0.0132, -0.0538, -0.0246],\n",
      "         [ 0.0278, -0.0409,  0.0048,  ..., -0.0202, -0.0784, -0.0504],\n",
      "         [ 0.0482, -0.0365, -0.0121,  ..., -0.0233, -0.0540, -0.0602],\n",
      "         ...,\n",
      "         [ 0.0593, -0.0196, -0.0049,  ..., -0.0015, -0.0413, -0.0549],\n",
      "         [ 0.0396, -0.0120, -0.0042,  ...,  0.0016, -0.0352, -0.0613],\n",
      "         [ 0.0638, -0.0445, -0.0271,  ...,  0.0002, -0.0531, -0.0446]],\n",
      "\n",
      "        [[ 0.0383,  0.0617,  0.0782,  ...,  0.0416, -0.0357,  0.0544],\n",
      "         [ 0.0508,  0.0487,  0.0539,  ...,  0.0593, -0.0269,  0.0382],\n",
      "         [ 0.0409,  0.0654,  0.0497,  ...,  0.0474, -0.0081,  0.0421],\n",
      "         ...,\n",
      "         [ 0.0324,  0.0616,  0.0258,  ...,  0.0334, -0.0632,  0.0610],\n",
      "         [ 0.0474,  0.0559,  0.0652,  ...,  0.0420, -0.0459,  0.0524],\n",
      "         [ 0.0247,  0.0535,  0.0477,  ...,  0.0676, -0.0290,  0.0431]]],\n",
      "       grad_fn=<ViewBackward0>),\n",
      "output.shape--->torch.Size([128, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "class multi_head_attention(nn.Module):\n",
    "    def __init__(self,d_model,n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_combine = nn.Linear(d_model, d_model)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size, seq_len, _ = q.shape\n",
    "        q, k ,v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.d_k).permute(0, 2, 1, 3)  # (batch_size, n_heads, seq_len, d_k)\n",
    "        k = k.view(batch_size, seq_len, self.n_heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        v = v.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "        score = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)  # (batch_size, n_heads, seq_len, seq_len)\n",
    "        if mask is not None:\n",
    "            # mask = torch.tril(torch.ones(seq_len, seq_len,dtype=bool))\n",
    "            score = score.masked_fill(mask == 0, float('-inf'))  # 只看前面的\n",
    "        score = self.softmax(score) @ v\n",
    "        score = score.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.w_combine(score)\n",
    "        return output\n",
    "    \n",
    "mha = multi_head_attention(d_model, n_heads)\n",
    "output = mha(X, X, X)\n",
    "print(f\"output--->{output},\\noutput.shape--->{output.shape}\")  # (batch_size, seq_len, d_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12f910",
   "metadata": {},
   "source": [
    "#### Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b84a22ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self,vocab_size, d_model):\n",
    "        super(TokenEmbedding, self).__init__(vocab_size, d_model, padding_idx=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f41f62",
   "metadata": {},
   "source": [
    "#### Post Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7a318f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output--->tensor([[[-0.9353,  0.8711,  0.7479,  ...,  1.0006, -0.1504,  2.0920],\n",
      "         [ 1.8241,  1.1223,  1.4230,  ...,  0.2347, -1.7227,  2.1554],\n",
      "         [ 0.1815, -1.1800,  2.5106,  ...,  0.2161,  1.9291,  0.7145],\n",
      "         ...,\n",
      "         [-1.7381,  0.3854,  2.0538,  ...,  2.3049,  0.7007,  1.2737],\n",
      "         [-0.5405, -0.0754,  0.2926,  ...,  1.3374, -0.2684,  0.3385],\n",
      "         [-0.1564,  1.8902, -1.6382,  ...,  0.5211, -0.3646,  0.3770]],\n",
      "\n",
      "        [[-0.1989, -0.5804,  0.9845,  ...,  0.8966,  2.1996,  1.3870],\n",
      "         [ 2.3472,  0.1405, -0.3111,  ...,  1.8604, -2.4045, -0.5477],\n",
      "         [ 0.9121, -1.6780,  0.9404,  ...,  1.7230, -0.2655,  2.0115],\n",
      "         ...,\n",
      "         [ 0.0090, -1.0691,  1.5351,  ...,  1.1705, -1.4624,  1.2464],\n",
      "         [ 0.2031,  1.7874,  1.5162,  ...,  1.3028, -0.4745,  1.0997],\n",
      "         [ 0.7151,  0.2676, -1.2182,  ...,  1.3420, -0.3032,  0.5342]],\n",
      "\n",
      "        [[ 1.2044,  0.7288,  0.3654,  ...,  0.9930, -1.4187,  0.0511],\n",
      "         [ 2.0304,  0.5797,  1.4508,  ...,  0.8744,  0.0694,  1.9682],\n",
      "         [ 0.6490,  0.0793,  2.6002,  ..., -0.4799, -1.6987, -0.8114],\n",
      "         ...,\n",
      "         [-0.6639, -2.5749,  2.4593,  ...,  1.6715,  0.2101, -0.1918],\n",
      "         [-1.3515,  0.5279,  0.4119,  ...,  1.2094,  0.0361,  2.0443],\n",
      "         [-1.0837,  1.8845,  0.2607,  ...,  0.0430,  1.0674,  0.0587]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4381, -0.3610,  0.2094,  ...,  2.7046,  1.7176,  0.5970],\n",
      "         [ 1.7033, -1.3269, -1.0528,  ...,  1.7864, -1.0879,  1.6700],\n",
      "         [ 0.3402,  1.2237,  0.1924,  ...,  0.6975, -0.2512,  0.7396],\n",
      "         ...,\n",
      "         [ 0.0484,  0.1757,  0.8525,  ...,  1.1282, -0.3686,  0.2359],\n",
      "         [-1.1732,  1.6848,  1.1127,  ...,  1.1595,  0.3612,  1.4601],\n",
      "         [-0.0222,  1.6162,  1.3513,  ...,  1.1538,  0.2183, -0.1748]],\n",
      "\n",
      "        [[ 0.4564,  0.3500,  0.1745,  ...,  1.2311, -0.8979,  1.6310],\n",
      "         [-0.0837, -0.0177,  1.2337,  ...,  0.5343, -0.4661,  0.4022],\n",
      "         [ 2.4089, -0.1844,  0.5015,  ...,  0.5854,  1.3204,  0.4192],\n",
      "         ...,\n",
      "         [-1.2398, -1.4616,  2.7828,  ...,  1.4906, -0.1688,  1.0953],\n",
      "         [-0.6071,  1.0301, -0.5262,  ..., -0.2700,  0.6189,  0.8830],\n",
      "         [ 0.9058,  0.3525, -1.1198,  ..., -0.1226,  1.1056,  0.4041]],\n",
      "\n",
      "        [[ 0.4420,  1.5915,  0.4933,  ...,  0.9314, -0.3230,  0.9837],\n",
      "         [ 1.8934,  0.9227,  0.6499,  ...,  0.8117,  0.3142, -0.6331],\n",
      "         [ 1.1308, -2.9048, -0.8668,  ...,  0.8597, -0.5044,  1.8112],\n",
      "         ...,\n",
      "         [-0.8599, -1.0801,  1.2817,  ...,  1.1222,  0.1047,  0.2332],\n",
      "         [-2.2064,  0.2240,  1.7364,  ..., -0.7926, -0.4032,  0.8796],\n",
      "         [-0.7621,  1.3251, -1.6523,  ...,  0.2571, -0.0255,  0.2993]]]),\n",
      "output.shape--->torch.Size([128, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        _2i = torch.arange(0, d_model, 2).float()\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:x.size(1), :].to(x.device)\n",
    "    \n",
    "pos_enc = PositionEncoding(d_model)\n",
    "output = pos_enc(X)\n",
    "print(f\"output--->{output},\\noutput.shape--->{output.shape}\")  # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46dd4b",
   "metadata": {},
   "source": [
    "#### Total Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aff7ff35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output--->tensor([[[-2.6721e+00,  2.0651e+00, -1.3672e-01,  ...,  7.1631e-01,\n",
      "          -4.8644e-01, -3.1298e+00],\n",
      "         [ 2.9003e+00,  1.6272e+00,  7.7190e-01,  ...,  4.5478e+00,\n",
      "          -7.8348e-01,  2.5305e+00],\n",
      "         [ 1.6634e+00, -6.4350e-01, -1.5827e+00,  ...,  8.6007e-01,\n",
      "           8.3142e-01,  1.2924e+00],\n",
      "         ...,\n",
      "         [-3.8266e+00, -3.0057e+00, -1.1582e+00,  ...,  2.4564e+00,\n",
      "           2.5342e+00,  3.6748e-01],\n",
      "         [ 2.5169e+00,  4.8155e+00, -4.1446e+00,  ...,  1.7320e+00,\n",
      "          -5.1551e+00,  3.2759e+00],\n",
      "         [ 4.6657e-01,  5.1061e+00, -2.8093e+00,  ..., -1.1008e-01,\n",
      "          -3.3807e+00, -1.8950e+00]],\n",
      "\n",
      "        [[ 7.3553e-01, -2.6307e-01, -7.9150e-02,  ...,  1.1460e+00,\n",
      "           2.1650e+00,  5.4773e-01],\n",
      "         [ 1.7295e+00,  3.7172e-01,  3.1760e+00,  ...,  4.0051e+00,\n",
      "          -3.2645e-01, -6.7654e-01],\n",
      "         [ 1.3691e+00, -3.7171e+00,  3.3594e+00,  ..., -7.5549e-01,\n",
      "          -1.6906e+00,  2.7740e+00],\n",
      "         ...,\n",
      "         [-3.0643e+00, -3.2989e+00,  1.9534e+00,  ..., -1.6493e+00,\n",
      "          -1.2238e+00,  1.5319e+00],\n",
      "         [-1.0236e+00, -3.0323e+00,  4.7901e-01,  ...,  1.4706e+00,\n",
      "          -1.1801e+00,  5.9024e-01],\n",
      "         [ 1.9670e+00,  2.4343e-01,  4.0855e-01,  ...,  4.6933e+00,\n",
      "          -1.4100e+00, -2.3370e-01]],\n",
      "\n",
      "        [[-6.0666e-01, -7.5048e-01,  9.7670e-01,  ...,  4.9818e+00,\n",
      "          -2.1735e-01,  2.3611e+00],\n",
      "         [ 1.7068e+00,  1.4691e+00, -8.4163e-01,  ...,  1.5340e+00,\n",
      "           3.0393e+00,  4.9161e+00],\n",
      "         [ 1.3651e+00, -2.8648e+00,  9.0095e-01,  ..., -2.2313e+00,\n",
      "          -2.8999e-01, -5.0605e-01],\n",
      "         ...,\n",
      "         [-5.4380e+00,  2.6026e+00,  2.1568e+00,  ..., -4.3833e+00,\n",
      "           8.3956e-01,  2.2088e+00],\n",
      "         [ 1.6010e+00,  4.1747e+00,  2.4847e+00,  ...,  7.0411e-01,\n",
      "           4.7382e-01, -2.6102e+00],\n",
      "         [ 1.4056e+00,  3.3903e+00, -7.7231e-01,  ...,  1.1675e-01,\n",
      "          -2.1595e-01,  1.5437e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.1601e+00,  2.5863e+00, -2.6453e+00,  ..., -3.9892e+00,\n",
      "          -5.2889e+00,  7.8123e-01],\n",
      "         [ 3.5273e+00,  5.1809e-01, -1.0813e+00,  ...,  1.0709e+00,\n",
      "          -6.7597e-01, -8.6662e-02],\n",
      "         [ 1.8366e+00,  6.9578e-01,  3.5021e+00,  ...,  1.1382e+00,\n",
      "          -1.1220e+00,  1.1219e+00],\n",
      "         ...,\n",
      "         [-2.1242e+00, -1.9552e-01,  2.3112e-03,  ...,  3.5383e+00,\n",
      "          -1.1959e+00, -1.8864e+00],\n",
      "         [-3.7511e+00,  2.5344e+00, -4.4764e+00,  ...,  6.6327e-01,\n",
      "          -1.7125e+00, -4.0573e-01],\n",
      "         [-8.7650e-01,  4.9391e+00,  9.5215e-01,  ..., -5.0260e-02,\n",
      "           1.5737e+00,  2.6004e+00]],\n",
      "\n",
      "        [[-3.9553e-01, -1.3270e+00, -3.7127e+00,  ...,  1.0819e+00,\n",
      "          -1.5501e-01, -1.0121e+00],\n",
      "         [-1.5385e+00, -1.7859e+00,  3.8410e+00,  ...,  1.7694e+00,\n",
      "          -3.4487e+00,  2.8437e+00],\n",
      "         [ 1.9767e+00,  1.1727e+00, -1.1517e+00,  ..., -4.0745e-01,\n",
      "           7.4635e-01,  1.4773e+00],\n",
      "         ...,\n",
      "         [-3.6614e+00,  4.1918e-01,  2.2507e-01,  ...,  1.7514e+00,\n",
      "           1.4660e+00,  2.3962e-01],\n",
      "         [-4.8028e+00,  1.6659e+00, -2.3321e+00,  ..., -2.0487e-02,\n",
      "          -6.3941e-01,  1.4428e+00],\n",
      "         [-7.5064e-02, -5.4018e-01, -2.8206e+00,  ...,  2.8073e+00,\n",
      "          -3.2882e-01,  2.6312e-01]],\n",
      "\n",
      "        [[ 1.2978e+00, -2.7291e-01,  6.1172e-02,  ...,  9.0785e-01,\n",
      "          -2.0716e+00,  3.2744e+00],\n",
      "         [ 6.0056e-01, -2.1213e+00,  3.4146e+00,  ...,  2.4715e+00,\n",
      "           2.0345e-01,  8.3803e-01],\n",
      "         [ 2.9956e+00, -1.7910e-01,  3.0824e+00,  ...,  2.6277e+00,\n",
      "           2.5828e+00,  1.6919e+00],\n",
      "         ...,\n",
      "         [-3.1548e+00,  2.2177e+00,  3.7831e+00,  ...,  1.9032e+00,\n",
      "           2.9809e-01,  5.0411e-01],\n",
      "         [-7.8635e-01,  2.7471e+00, -3.8550e+00,  ...,  3.6905e-01,\n",
      "           8.6821e-01,  1.2759e+00],\n",
      "         [ 1.1334e-01,  5.4615e+00,  5.3907e-01,  ...,  1.2135e+00,\n",
      "          -1.6766e+00,  2.8307e+00]]], grad_fn=<AddBackward0>),\n",
      "output.shape--->torch.Size([128, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, dropout=0.1):\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.token_embedding = TokenEmbedding(vocab_size, d_model)\n",
    "        self.position_encoding = PositionEncoding(d_model, max_len)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_encoding(token_emb)\n",
    "        return pos_emb+ token_emb\n",
    "    \n",
    "transformer_emb = TransformerEmbedding(vocab_size=10000, d_model=d_model, max_len=5000)\n",
    "output = transformer_emb(torch.randint(0, 10000, (128, 64)))\n",
    "print(f\"output--->{output},\\noutput.shape--->{output.shape}\")  # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4c977f",
   "metadata": {},
   "source": [
    "#### LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2099da82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output--->tensor([[[-8.2814e-01, -1.1455e-02,  8.7660e-01,  ...,  1.1970e-01,\n",
      "          -3.3195e-02,  1.2250e+00],\n",
      "         [ 8.5700e-01,  4.7555e-01,  4.9383e-01,  ..., -8.0727e-01,\n",
      "          -1.7190e+00,  1.0216e+00],\n",
      "         [-6.5263e-01, -6.8685e-01,  1.5315e+00,  ..., -7.0587e-01,\n",
      "           1.8680e+00, -2.3293e-01],\n",
      "         ...,\n",
      "         [-8.3752e-01,  6.6420e-01,  1.3662e+00,  ...,  1.3659e+00,\n",
      "           7.1810e-01,  2.7189e-01],\n",
      "         [ 1.7974e-01, -7.7588e-01,  3.9393e-01,  ...,  3.1958e-01,\n",
      "          -2.9778e-01, -6.8774e-01],\n",
      "         [-2.9427e-01,  9.3563e-01, -7.2572e-01,  ..., -4.4955e-01,\n",
      "          -3.4167e-01, -5.9390e-01]],\n",
      "\n",
      "        [[-1.7947e-01, -1.5378e+00,  9.8403e-01,  ..., -8.5618e-02,\n",
      "           2.1788e+00,  3.9657e-01],\n",
      "         [ 1.5262e+00, -3.1626e-01, -1.0251e+00,  ...,  9.0219e-01,\n",
      "          -2.2546e+00, -1.4261e+00],\n",
      "         [-5.4766e-02, -1.3056e+00, -5.3590e-02,  ...,  6.5764e-01,\n",
      "          -3.2030e-01,  9.4295e-01],\n",
      "         ...,\n",
      "         [ 9.8317e-01, -8.1928e-01,  7.9288e-01,  ...,  1.7120e-01,\n",
      "          -1.4830e+00,  2.4779e-01],\n",
      "         [ 8.9273e-01,  1.0688e+00,  1.6032e+00,  ...,  2.3677e-01,\n",
      "          -5.6734e-01,  2.8391e-02],\n",
      "         [ 5.3293e-01, -7.6762e-01, -3.7342e-01,  ...,  3.2164e-01,\n",
      "          -3.4792e-01, -5.0815e-01]],\n",
      "\n",
      "        [[ 1.1089e+00, -3.3856e-01,  2.8590e-01,  ..., -7.9398e-02,\n",
      "          -1.4642e+00, -1.0033e+00],\n",
      "         [ 1.2357e+00,  6.0837e-02,  6.6339e-01,  ..., -1.0785e-01,\n",
      "           9.1390e-02,  1.0101e+00],\n",
      "         [-1.9247e-01,  5.6040e-01,  1.7243e+00,  ..., -1.4074e+00,\n",
      "          -1.6256e+00, -1.7376e+00],\n",
      "         ...,\n",
      "         [ 2.8627e-01, -2.3526e+00,  1.7055e+00,  ...,  6.5837e-01,\n",
      "           1.8710e-01, -1.2191e+00],\n",
      "         [-7.0863e-01, -2.2268e-01,  4.8118e-01,  ...,  1.4699e-01,\n",
      "          -4.0227e-02,  1.0162e+00],\n",
      "         [-1.3239e+00,  8.5110e-01,  1.0997e+00,  ..., -1.0264e+00,\n",
      "           1.0152e+00, -1.0105e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.3852e-01, -1.3544e+00,  2.0401e-01,  ...,  1.6879e+00,\n",
      "           1.7008e+00, -4.0366e-01],\n",
      "         [ 8.4840e-01, -1.9529e+00, -1.9606e+00,  ...,  7.7094e-01,\n",
      "          -1.1531e+00,  6.5144e-01],\n",
      "         [-5.6787e-01,  1.6448e+00, -7.4305e-01,  ..., -3.0083e-01,\n",
      "          -2.4968e-01, -2.5862e-01],\n",
      "         ...,\n",
      "         [ 9.9681e-01,  4.0742e-01,  7.2620e-02,  ...,  9.7271e-02,\n",
      "          -4.1337e-01, -8.0828e-01],\n",
      "         [-5.0689e-01,  8.7921e-01,  1.0901e+00,  ...,  6.2390e-02,\n",
      "           2.4961e-01,  3.5063e-01],\n",
      "         [-1.7275e-01,  6.6500e-01,  2.3044e+00,  ...,  1.7815e-01,\n",
      "           2.3728e-01, -1.1794e+00]],\n",
      "\n",
      "        [[ 4.1961e-01, -6.6973e-01,  1.4207e-01,  ...,  1.9778e-01,\n",
      "          -9.1383e-01,  5.9161e-01],\n",
      "         [-9.3159e-01, -5.6011e-01,  4.2121e-01,  ..., -4.6673e-01,\n",
      "          -4.6726e-01, -6.0041e-01],\n",
      "         [ 1.4705e+00,  2.2010e-01, -4.3742e-01,  ..., -4.1740e-01,\n",
      "           1.2936e+00, -5.8124e-01],\n",
      "         ...,\n",
      "         [-2.8846e-01, -1.2568e+00,  2.1153e+00,  ...,  5.0759e-01,\n",
      "          -1.8576e-01,  9.5942e-02],\n",
      "         [ 8.9971e-02,  3.1345e-01, -4.4718e-01,  ..., -1.3054e+00,\n",
      "           5.6813e-01, -1.5790e-01],\n",
      "         [ 8.0564e-01, -6.1170e-01, -2.0135e-01,  ..., -1.1170e+00,\n",
      "           1.1783e+00, -5.7295e-01]],\n",
      "\n",
      "        [[ 4.6532e-01,  6.1775e-01,  5.1757e-01,  ..., -5.5578e-02,\n",
      "          -3.1501e-01, -2.2219e-03],\n",
      "         [ 1.0621e+00,  3.8628e-01, -1.7322e-01,  ..., -1.8973e-01,\n",
      "           3.1734e-01, -1.6480e+00],\n",
      "         [ 1.6954e-01, -2.4529e+00, -1.7897e+00,  ..., -1.8060e-01,\n",
      "          -5.3309e-01,  7.4010e-01],\n",
      "         ...,\n",
      "         [ 1.0704e-01, -8.2575e-01,  5.3617e-01,  ...,  1.2314e-01,\n",
      "           9.9201e-02, -7.7021e-01],\n",
      "         [-1.3661e+00, -3.8722e-01,  1.8293e+00,  ..., -1.6790e+00,\n",
      "          -3.4884e-01, -7.0656e-02],\n",
      "         [-8.9038e-01,  3.4784e-01, -7.3349e-01,  ..., -7.0826e-01,\n",
      "          -1.4432e-02, -6.6713e-01]]], grad_fn=<AddBackward0>),\n",
      "output.shape--->torch.Size([128, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim=-1,keepdim=True)\n",
    "        std = x.std(dim=-1,keepdim=True)\n",
    "        x = (x-mean) / (std + self.eps)\n",
    "        return self.gamma * x + self.beta\n",
    "\n",
    "ln = LayerNorm(d_model)\n",
    "output = ln(X)\n",
    "print(f\"output--->{output},\\noutput.shape--->{output.shape}\")  # (batch_size, seq_len, d_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c18ace2",
   "metadata": {},
   "source": [
    "#### FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b46a3503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output--->tensor([[[-0.1287,  0.2055, -0.0140,  ...,  0.1522, -0.3233,  0.0281],\n",
      "         [-0.0809,  0.3722,  0.3491,  ...,  0.3314, -0.4500, -0.0104],\n",
      "         [ 0.1162, -0.2394,  0.3441,  ..., -0.2541, -0.1235,  0.3885],\n",
      "         ...,\n",
      "         [-0.0984, -0.0346,  0.3007,  ..., -0.0445, -0.4175, -0.0134],\n",
      "         [-0.1214,  0.0887,  0.2027,  ...,  0.1193, -0.2498,  0.1965],\n",
      "         [-0.1386, -0.2615,  0.0962,  ...,  0.4148, -0.1505,  0.1376]],\n",
      "\n",
      "        [[ 0.1689, -0.2057, -0.0895,  ..., -0.3432, -0.5748, -0.2058],\n",
      "         [-0.1668, -0.3674, -0.3573,  ...,  0.0699, -0.5165,  0.1554],\n",
      "         [ 0.2569, -0.7297, -0.0336,  ..., -0.2967, -0.1650, -0.0086],\n",
      "         ...,\n",
      "         [-0.1440, -0.0537,  0.0292,  ..., -0.0751, -0.6167, -0.1168],\n",
      "         [-0.2178, -0.1165, -0.3610,  ...,  0.4018, -0.4244,  0.4999],\n",
      "         [ 0.1494, -0.0570,  0.3570,  ...,  0.4982, -0.0237,  0.1025]],\n",
      "\n",
      "        [[-0.1903,  0.0579,  0.1035,  ..., -0.0476, -0.3276,  0.1784],\n",
      "         [ 0.0147, -0.2066, -0.3353,  ...,  0.1539, -0.1162,  0.2914],\n",
      "         [ 0.2922, -0.2432,  0.1402,  ...,  0.2065,  0.0558,  0.2677],\n",
      "         ...,\n",
      "         [-0.1398, -0.0817, -0.1327,  ...,  0.3252,  0.0994,  0.2685],\n",
      "         [ 0.4344, -0.3203,  0.0540,  ...,  0.3235,  0.2241,  0.4056],\n",
      "         [ 0.0987, -0.0187,  0.0490,  ..., -0.1973, -0.6100,  0.3215]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0378,  0.3519, -0.3707,  ...,  0.0995,  0.0541,  0.1244],\n",
      "         [ 0.1753, -0.3717, -0.2805,  ...,  0.1085, -0.3408,  0.1710],\n",
      "         [-0.2794, -0.3274, -0.3057,  ...,  0.2179,  0.0503,  0.1645],\n",
      "         ...,\n",
      "         [-0.1287, -0.4021, -0.4664,  ...,  0.1090, -0.2159,  0.3431],\n",
      "         [-0.2379, -0.0304, -0.0850,  ...,  0.2113, -0.2882,  0.2892],\n",
      "         [ 0.3010, -0.4358, -0.2495,  ...,  0.2823, -0.2749,  0.3023]],\n",
      "\n",
      "        [[ 0.2390,  0.3036,  0.0132,  ...,  0.2144, -0.3502,  0.3405],\n",
      "         [ 0.1355,  0.1903, -0.2460,  ..., -0.1208,  0.0959, -0.1264],\n",
      "         [-0.1227,  0.2712,  0.1891,  ...,  0.1500, -0.4294,  0.0024],\n",
      "         ...,\n",
      "         [ 0.1520,  0.1457,  0.0220,  ...,  0.0302, -0.4440, -0.0670],\n",
      "         [ 0.0661, -0.4491, -0.0796,  ...,  0.1438, -0.3245,  0.2035],\n",
      "         [ 0.2634, -0.0363, -0.1443,  ..., -0.1188, -0.6475, -0.3089]],\n",
      "\n",
      "        [[-0.1843,  0.0307,  0.4106,  ...,  0.3751,  0.1887, -0.0476],\n",
      "         [ 0.1122,  0.2229, -0.1828,  ...,  0.1884, -0.0482,  0.0958],\n",
      "         [ 0.3199, -0.2418, -0.2276,  ...,  0.4024, -0.0844, -0.2479],\n",
      "         ...,\n",
      "         [-0.1821,  0.0168, -0.2300,  ..., -0.2505, -0.2032, -0.0196],\n",
      "         [ 0.0945, -0.0979, -0.1502,  ...,  0.2653, -0.0743,  0.1108],\n",
      "         [-0.1081, -0.0578, -0.3890,  ...,  0.0623, -0.3435,  0.5278]]],\n",
      "       grad_fn=<ViewBackward0>),\n",
      "output.shape--->torch.Size([128, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.w_1(x)))\n",
    "        x = self.w_2(x)\n",
    "        return x\n",
    "    \n",
    "ffn = PositionwiseFeedForward(d_model)\n",
    "output = ffn(X)\n",
    "print(f\"output--->{output},\\noutput.shape--->{output.shape}\")  # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c38759",
   "metadata": {},
   "source": [
    "#### EncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6400b52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output--->tensor([[[-7.0603e-01,  5.7553e-01,  7.5261e-01,  ..., -2.0858e-01,\n",
      "           5.8189e-02,  1.1278e+00],\n",
      "         [ 1.1365e+00,  6.0335e-01,  8.4254e-01,  ..., -5.2876e-01,\n",
      "          -1.6772e+00,  1.2292e+00],\n",
      "         [-7.5794e-02, -6.9841e-01,  1.2713e+00,  ..., -7.7817e-01,\n",
      "           2.0499e+00, -2.0246e-01],\n",
      "         ...,\n",
      "         [-3.7518e-01,  2.6045e-01,  1.3156e+00,  ...,  1.0443e+00,\n",
      "           2.9703e-01,  5.3704e-03],\n",
      "         [ 1.6703e-01, -1.0393e+00,  4.4509e-01,  ..., -3.8164e-02,\n",
      "           9.4134e-02, -3.0491e-01],\n",
      "         [-7.9923e-04,  7.0837e-01, -6.2037e-01,  ..., -4.2584e-01,\n",
      "          -3.6852e-01, -6.1785e-01]],\n",
      "\n",
      "        [[-1.7826e-01, -1.5271e+00,  1.4500e+00,  ..., -5.7538e-01,\n",
      "           2.3379e+00,  5.8379e-01],\n",
      "         [ 1.6827e+00, -2.9564e-01, -1.0193e+00,  ...,  1.0930e+00,\n",
      "          -2.0300e+00, -1.1677e+00],\n",
      "         [-5.4246e-02, -1.3351e+00, -1.5155e-02,  ...,  7.9079e-01,\n",
      "          -7.9953e-01,  6.9293e-01],\n",
      "         ...,\n",
      "         [ 1.0194e+00, -8.3895e-01,  8.8811e-01,  ...,  2.4811e-01,\n",
      "          -1.5490e+00,  5.1607e-01],\n",
      "         [ 8.6230e-01,  1.1996e+00,  1.6389e+00,  ...,  2.9548e-01,\n",
      "          -4.9494e-01,  2.8815e-02],\n",
      "         [ 4.3306e-01, -5.0847e-01, -3.0699e-01,  ...,  1.7704e-01,\n",
      "          -2.5284e-01, -4.1207e-01]],\n",
      "\n",
      "        [[ 1.5176e+00, -2.5102e-01,  7.2705e-01,  ..., -2.0473e-01,\n",
      "          -1.8712e+00, -1.1816e+00],\n",
      "         [ 1.3503e+00,  2.5084e-01,  8.3158e-01,  ..., -7.8054e-01,\n",
      "           1.8898e-01,  9.0975e-01],\n",
      "         [-3.4759e-02,  5.2486e-01,  1.9449e+00,  ..., -1.5482e+00,\n",
      "          -1.6568e+00, -1.5423e+00],\n",
      "         ...,\n",
      "         [ 5.9987e-01, -2.0930e+00,  1.8077e+00,  ...,  7.2888e-01,\n",
      "           4.7168e-02, -1.1930e+00],\n",
      "         [-6.2082e-01, -1.8130e-02,  5.0607e-01,  ...,  1.3893e-01,\n",
      "           1.2433e-01,  9.1815e-01],\n",
      "         [-1.2716e+00,  1.0971e+00,  1.0110e+00,  ..., -1.2839e+00,\n",
      "           9.7842e-01, -9.8841e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.1499e-01, -1.0896e+00,  2.0321e-01,  ...,  1.1796e+00,\n",
      "           1.3605e+00, -3.4447e-01],\n",
      "         [ 7.9517e-01, -2.4350e+00, -1.9183e+00,  ...,  6.1385e-01,\n",
      "          -1.4600e+00,  8.4792e-01],\n",
      "         [-6.3835e-01,  1.9139e+00, -4.9337e-01,  ..., -2.7842e-01,\n",
      "          -1.1787e-01, -1.6128e-01],\n",
      "         ...,\n",
      "         [ 1.4347e+00,  3.7311e-01,  1.5446e-01,  ..., -3.4330e-01,\n",
      "          -4.4145e-01, -7.2531e-01],\n",
      "         [-3.3148e-01,  8.1913e-01,  9.3749e-01,  ..., -5.0695e-02,\n",
      "           3.3792e-01,  3.2463e-01],\n",
      "         [ 2.0692e-01,  4.3593e-01,  1.8889e+00,  ...,  1.9208e-01,\n",
      "          -2.0232e-01, -1.0430e+00]],\n",
      "\n",
      "        [[ 2.3791e-01, -5.8735e-01,  3.2403e-01,  ..., -7.2597e-02,\n",
      "          -5.2757e-01,  8.2087e-01],\n",
      "         [-8.1947e-01, -8.5295e-01,  1.8523e-01,  ..., -7.6427e-01,\n",
      "          -3.8814e-01, -7.2415e-02],\n",
      "         [ 1.9044e+00,  7.5588e-01, -5.3878e-01,  ..., -7.1694e-01,\n",
      "           1.2418e+00, -4.0031e-01],\n",
      "         ...,\n",
      "         [-2.6796e-01, -1.3813e+00,  1.7878e+00,  ...,  3.2816e-01,\n",
      "           2.9224e-02,  3.2770e-01],\n",
      "         [ 5.0153e-01,  3.6606e-01, -3.1679e-01,  ..., -1.3712e+00,\n",
      "           6.7491e-01, -1.6237e-01],\n",
      "         [ 1.0073e+00, -7.8382e-01, -4.2061e-02,  ..., -1.0903e+00,\n",
      "           1.0472e+00, -6.3085e-01]],\n",
      "\n",
      "        [[ 3.4329e-01,  1.5767e-01,  3.1308e-01,  ..., -3.3251e-01,\n",
      "          -1.1815e-01,  5.2092e-02],\n",
      "         [ 1.3759e+00,  4.8370e-01, -4.2458e-02,  ..., -3.3185e-01,\n",
      "           4.0597e-01, -1.3329e+00],\n",
      "         [ 6.2485e-01, -2.0742e+00, -1.7571e+00,  ..., -2.5900e-01,\n",
      "          -3.5985e-01,  7.8319e-02],\n",
      "         ...,\n",
      "         [ 5.8269e-01, -9.8493e-01,  6.0500e-01,  ..., -3.1541e-01,\n",
      "           3.6850e-01, -6.7475e-01],\n",
      "         [-1.3303e+00, -4.6541e-01,  1.8378e+00,  ..., -2.2770e+00,\n",
      "          -3.6003e-01, -1.0773e-01],\n",
      "         [-5.3369e-01,  3.0824e-01, -6.8312e-01,  ..., -9.6053e-01,\n",
      "           2.9868e-02, -9.3057e-01]]], grad_fn=<AddBackward0>),\n",
      "output.shape--->torch.Size([128, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = multi_head_attention(d_model, n_heads)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x+self.dropout(attn_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x+self.dropout(ffn_output))\n",
    "        return x\n",
    "    \n",
    "encoder_layer = EncoderLayer(d_model, n_heads)\n",
    "output = encoder_layer(X)\n",
    "print(f\"output--->{output},\\noutput.shape--->{output.shape}\")  # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f116ed2",
   "metadata": {},
   "source": [
    "#### DecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f0bd50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output--->tensor([[[-1.1896e+00, -7.0887e-02,  6.6363e-01,  ...,  4.0739e-01,\n",
      "           3.4183e-01,  1.0413e+00],\n",
      "         [ 8.8248e-01,  1.7339e-01,  3.7401e-01,  ..., -1.0262e+00,\n",
      "          -1.1995e+00,  8.1794e-01],\n",
      "         [-3.6680e-01, -9.5613e-01,  1.3482e+00,  ..., -5.8089e-01,\n",
      "           2.3191e+00, -3.3271e-01],\n",
      "         ...,\n",
      "         [-4.5736e-01,  5.4850e-01,  8.2544e-01,  ...,  1.3540e+00,\n",
      "           9.9084e-01, -1.6764e-01],\n",
      "         [ 3.3166e-01, -7.0236e-01, -3.0535e-01,  ...,  3.0848e-01,\n",
      "           2.1121e-01, -6.5877e-01],\n",
      "         [-1.7814e-01,  1.0016e+00, -6.4724e-01,  ..., -4.5331e-01,\n",
      "           1.9896e-02, -7.8978e-01]],\n",
      "\n",
      "        [[-1.1991e-02, -1.8435e+00,  8.8692e-01,  ...,  1.8697e-01,\n",
      "           2.3730e+00,  2.4594e-01],\n",
      "         [ 1.9889e+00, -5.6041e-01, -1.2818e+00,  ...,  8.6363e-01,\n",
      "          -1.7861e+00, -1.7457e+00],\n",
      "         [-4.4614e-02, -1.6247e+00, -4.2892e-01,  ...,  8.1105e-01,\n",
      "          -3.1793e-01,  8.5236e-01],\n",
      "         ...,\n",
      "         [ 1.2534e+00, -8.3319e-01,  8.5077e-02,  ...,  4.8785e-01,\n",
      "          -1.0456e+00, -2.9032e-01],\n",
      "         [ 1.1983e+00,  1.0046e+00,  1.6574e+00,  ...,  4.7882e-02,\n",
      "          -4.4481e-01, -5.3238e-01],\n",
      "         [ 2.3290e-01, -9.9703e-01, -6.5663e-01,  ...,  5.0369e-01,\n",
      "          -3.3821e-01, -1.1044e+00]],\n",
      "\n",
      "        [[ 1.5767e+00, -7.1531e-01,  3.2735e-01,  ..., -6.8800e-02,\n",
      "          -1.3712e+00, -1.3114e+00],\n",
      "         [ 1.3681e+00,  3.4880e-02,  4.1785e-01,  ...,  2.0936e-01,\n",
      "           1.6398e-01,  1.0080e+00],\n",
      "         [ 9.8012e-02,  2.5549e-01,  1.4897e+00,  ..., -1.1819e+00,\n",
      "          -1.0143e+00, -2.5566e+00],\n",
      "         ...,\n",
      "         [ 4.2834e-01, -2.4945e+00,  1.5920e+00,  ...,  7.1336e-01,\n",
      "           2.3244e-01, -1.5103e+00],\n",
      "         [-2.9032e-01, -5.9530e-01,  5.6825e-01,  ...,  5.3307e-02,\n",
      "           1.1462e-01,  5.8072e-01],\n",
      "         [-9.9040e-01,  4.5777e-01,  9.7219e-01,  ..., -4.7192e-01,\n",
      "           1.1254e+00, -1.3598e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.4556e-01, -1.5105e+00, -5.5328e-01,  ...,  1.5510e+00,\n",
      "           1.5441e+00, -6.0434e-01],\n",
      "         [ 6.1145e-01, -2.4975e+00, -2.2193e+00,  ...,  7.1612e-01,\n",
      "          -7.6385e-01,  4.9303e-01],\n",
      "         [ 4.0665e-03,  1.3749e+00, -7.7769e-01,  ..., -1.9261e-01,\n",
      "           2.5216e-01,  9.9993e-02],\n",
      "         ...,\n",
      "         [ 1.3919e+00,  3.5662e-01,  8.9637e-02,  ...,  1.0351e-01,\n",
      "          -1.6022e-03, -9.7731e-01],\n",
      "         [-3.0890e-01,  3.0979e-01,  1.4452e+00,  ...,  2.9936e-01,\n",
      "           2.7603e-01,  1.1901e-01],\n",
      "         [-3.5306e-02,  5.1923e-01,  2.1829e+00,  ...,  2.6172e-01,\n",
      "           3.9320e-01, -1.2329e+00]],\n",
      "\n",
      "        [[ 6.3503e-01, -7.0404e-01, -3.1888e-01,  ...,  2.6869e-01,\n",
      "          -6.3273e-01,  8.4647e-02],\n",
      "         [-8.8010e-01, -6.9846e-01,  5.0592e-01,  ..., -4.5246e-01,\n",
      "          -2.6850e-01, -7.1890e-01],\n",
      "         [ 1.3953e+00, -3.7145e-02, -3.2373e-01,  ..., -4.2442e-01,\n",
      "           1.3706e+00, -1.3645e+00],\n",
      "         ...,\n",
      "         [ 1.9174e-01, -1.5699e+00,  1.9491e+00,  ...,  2.5197e-01,\n",
      "           4.1447e-01,  1.6150e-02],\n",
      "         [ 2.3080e-01, -2.8048e-01, -5.8330e-01,  ..., -1.0455e+00,\n",
      "           1.0960e+00, -4.3797e-01],\n",
      "         [ 9.4962e-01, -1.8615e-01, -9.5657e-02,  ..., -1.2328e+00,\n",
      "           1.1185e+00, -7.9736e-01]],\n",
      "\n",
      "        [[ 4.7132e-01,  1.9620e-01,  7.5718e-01,  ..., -2.5524e-01,\n",
      "           9.0911e-03, -2.9635e-01],\n",
      "         [ 1.3748e+00,  3.4513e-01, -2.8249e-01,  ..., -1.3865e-03,\n",
      "           1.2348e+00, -1.5630e+00],\n",
      "         [ 3.0914e-01, -2.3560e+00, -1.9318e+00,  ...,  4.8000e-02,\n",
      "          -2.9957e-01,  5.0540e-01],\n",
      "         ...,\n",
      "         [ 2.2171e-01, -1.0768e+00, -2.5961e-03,  ...,  4.6697e-01,\n",
      "           2.9174e-01, -7.8545e-01],\n",
      "         [-1.3457e+00, -7.2249e-01,  1.5909e+00,  ..., -1.9175e+00,\n",
      "          -1.2929e-01, -4.1060e-01],\n",
      "         [-4.2890e-01, -2.0702e-01, -8.0789e-01,  ..., -5.9608e-01,\n",
      "           5.5741e-01, -6.3932e-01]]], grad_fn=<AddBackward0>),\n",
      "output.shape--->torch.Size([128, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = multi_head_attention(d_model, n_heads)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.cross_attn = multi_head_attention(d_model, n_heads)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        self_attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(self_attn_output))\n",
    "        cross_attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout2(cross_attn_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm3(x + self.dropout3(ffn_output))\n",
    "        return x\n",
    "        \n",
    "decoder_layer = DecoderLayer(d_model, n_heads)\n",
    "output = decoder_layer(X, X)\n",
    "print(f\"output--->{output},\\noutput.shape--->{output.shape}\")  # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e96b71f",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6b25a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output--->tensor([[[ 9.5672e-01, -5.5659e-01, -5.1308e-01,  ...,  2.9756e-01,\n",
      "          -1.8059e-01,  1.3197e-01],\n",
      "         [-9.7916e-02, -1.3802e+00, -1.0517e-01,  ..., -6.2186e-01,\n",
      "           2.8005e-02, -4.2974e-01],\n",
      "         [ 3.7342e-01, -6.5561e-01,  1.4070e+00,  ..., -2.7220e-01,\n",
      "          -5.0496e-01,  8.1798e-01],\n",
      "         ...,\n",
      "         [ 8.7133e-01, -1.0461e+00,  2.0067e+00,  ..., -4.6159e-02,\n",
      "           1.2420e+00, -3.7341e-01],\n",
      "         [ 8.6260e-01, -2.6430e-01,  1.7339e+00,  ...,  6.5361e-02,\n",
      "          -5.2534e-01, -3.2787e-01],\n",
      "         [ 2.1095e+00,  4.9862e-01, -9.9248e-01,  ...,  1.4241e-01,\n",
      "          -3.5422e-01,  1.1507e+00]],\n",
      "\n",
      "        [[ 3.1536e-01, -4.4696e-01,  5.3404e-01,  ...,  5.9221e-01,\n",
      "          -6.4635e-03,  9.9375e-01],\n",
      "         [ 1.0559e+00,  1.1605e+00, -5.2131e-01,  ...,  1.3782e+00,\n",
      "          -1.4143e+00,  3.3445e-01],\n",
      "         [ 6.3440e-01, -9.9827e-01, -1.0822e+00,  ..., -8.4854e-01,\n",
      "          -5.7946e-01,  7.5415e-01],\n",
      "         ...,\n",
      "         [ 1.1659e+00, -6.9902e-01,  1.0309e+00,  ...,  9.1526e-01,\n",
      "          -1.5900e+00, -4.7085e-01],\n",
      "         [ 3.2433e-01,  2.8477e-01,  6.3223e-01,  ...,  1.4936e+00,\n",
      "          -8.7405e-03,  4.5068e-01],\n",
      "         [-5.7662e-01,  3.8696e-01,  7.2200e-01,  ...,  1.6937e+00,\n",
      "          -7.8678e-02,  5.9629e-01]],\n",
      "\n",
      "        [[-4.2103e-01, -6.7004e-01, -1.1010e+00,  ...,  2.1161e+00,\n",
      "          -2.8893e-01,  3.4527e-01],\n",
      "         [ 4.3885e-01, -1.7920e+00,  4.9957e-01,  ..., -1.0261e+00,\n",
      "          -1.0170e+00, -3.7006e-01],\n",
      "         [ 2.0599e-01, -8.5792e-01,  1.5167e+00,  ...,  1.1487e+00,\n",
      "           1.0308e+00, -4.5462e-01],\n",
      "         ...,\n",
      "         [ 7.5231e-02,  2.7865e-01,  7.0467e-01,  ...,  4.4095e-01,\n",
      "          -6.5193e-02,  8.2946e-01],\n",
      "         [ 1.3332e+00,  1.9529e+00,  1.0306e+00,  ...,  7.6979e-01,\n",
      "          -7.6052e-01,  1.1638e+00],\n",
      "         [ 5.6291e-01, -4.6751e-01,  4.4508e-01,  ...,  8.9796e-01,\n",
      "           9.6157e-01, -8.4063e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0703e+00,  1.6411e+00, -1.5340e+00,  ...,  5.7404e-02,\n",
      "          -2.8255e-01,  1.7169e-02],\n",
      "         [-1.7129e+00, -2.2254e-01,  9.8478e-01,  ..., -1.6300e-01,\n",
      "          -3.3394e-01,  8.8870e-01],\n",
      "         [ 4.8955e-01, -3.6863e-01,  7.8714e-01,  ...,  5.1903e-01,\n",
      "           1.7231e+00,  1.1337e+00],\n",
      "         ...,\n",
      "         [ 1.6866e+00, -8.5615e-01,  2.2929e+00,  ...,  1.1777e+00,\n",
      "          -2.1978e-01, -2.3767e-01],\n",
      "         [-9.3844e-01,  6.6192e-02, -1.2290e-03,  ...,  2.5918e+00,\n",
      "          -9.7065e-01, -5.5048e-01],\n",
      "         [ 3.0253e-01,  1.4349e+00,  9.0827e-01,  ...,  1.0290e+00,\n",
      "          -6.5712e-01,  7.1745e-01]],\n",
      "\n",
      "        [[ 8.3458e-01, -9.3985e-02,  2.6778e+00,  ...,  1.4375e+00,\n",
      "          -4.1722e-01, -6.1117e-01],\n",
      "         [-9.8354e-01,  2.0826e-01, -2.5554e-01,  ...,  1.4045e-01,\n",
      "          -1.2000e+00, -6.1621e-01],\n",
      "         [ 2.9259e+00, -1.0885e+00,  2.3530e-01,  ..., -1.3500e-01,\n",
      "           2.2717e-01, -1.6907e-01],\n",
      "         ...,\n",
      "         [ 5.5758e-01, -6.7185e-01,  1.4034e+00,  ...,  2.0799e+00,\n",
      "           4.8198e-01, -9.6785e-01],\n",
      "         [ 1.1765e+00,  1.4820e+00,  3.0178e-01,  ...,  1.2654e+00,\n",
      "           8.1374e-01,  5.4638e-01],\n",
      "         [ 8.9195e-01,  3.1592e-01, -3.5533e-02,  ..., -5.1805e-01,\n",
      "           1.8753e+00,  7.4171e-01]],\n",
      "\n",
      "        [[ 1.9830e-02, -1.3439e+00,  1.2171e+00,  ...,  3.8773e-01,\n",
      "          -1.4420e+00,  1.8631e+00],\n",
      "         [ 1.1044e+00,  1.5689e-01,  1.5636e+00,  ..., -4.6676e-01,\n",
      "          -9.5711e-01,  9.2273e-01],\n",
      "         [ 1.0929e+00, -1.1565e+00, -3.4494e-01,  ..., -9.2219e-01,\n",
      "          -1.2497e+00,  1.7217e+00],\n",
      "         ...,\n",
      "         [ 3.4251e-01, -6.9793e-01,  5.8568e-01,  ...,  2.6158e+00,\n",
      "          -1.6250e+00,  8.3493e-01],\n",
      "         [-1.3707e+00, -2.1683e+00,  9.3888e-02,  ...,  1.0864e+00,\n",
      "           1.3630e-01,  1.3560e+00],\n",
      "         [ 9.2800e-01,  4.2464e-02,  3.4376e-01,  ...,  1.1606e+00,\n",
      "          -1.5230e+00,  1.2603e+00]]], grad_fn=<AddBackward0>),\n",
      "output.shape--->torch.Size([128, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,vocab_size, d_model, n_layers, n_heads,  max_len, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = TransformerEmbedding(vocab_size, d_model, max_len)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
    "        self.norm = LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "encoder = Encoder(vocab_size=10000, d_model=d_model, n_layers=6, n_heads=n_heads, max_len=5000)\n",
    "output = encoder(torch.randint(0, 10000, (128, 64)))\n",
    "print(f\"output--->{output},\\noutput.shape--->{output.shape}\")  # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e21add",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9991632e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output--->tensor([[[ 0.4541, -0.5923,  0.6336,  ...,  1.5332, -0.6194,  2.1135],\n",
      "         [-1.4333,  0.8145,  0.3547,  ...,  0.1490,  0.3329,  0.6539],\n",
      "         [ 0.1714,  0.6863,  1.1653,  ..., -0.4941, -0.6235, -0.2031],\n",
      "         ...,\n",
      "         [-0.6739, -1.6603,  0.9431,  ...,  0.6502,  0.8397,  1.0983],\n",
      "         [ 0.6853, -1.3998, -0.3372,  ...,  0.7246,  0.3279,  0.2709],\n",
      "         [ 0.5900, -0.4388,  0.1294,  ..., -1.4263,  0.3875,  1.5885]],\n",
      "\n",
      "        [[-0.5721, -0.1229,  1.3940,  ..., -0.0215, -0.4064,  1.3293],\n",
      "         [ 0.4503, -0.8469,  0.6139,  ..., -0.6272,  0.4465,  0.9701],\n",
      "         [ 0.5181, -2.0120,  0.4316,  ..., -0.0662,  0.9880, -0.7793],\n",
      "         ...,\n",
      "         [-0.8555, -1.5487,  1.0834,  ...,  1.7692,  1.0356,  0.4158],\n",
      "         [-1.3909, -0.9398, -0.2683,  ..., -1.1808, -0.8694, -1.2517],\n",
      "         [ 0.9656,  0.2940, -1.3547,  ...,  0.6702,  0.6983,  0.2181]],\n",
      "\n",
      "        [[ 0.5427,  0.0380,  1.0362,  ..., -0.2925,  0.6481,  0.8463],\n",
      "         [ 0.0268, -1.9165, -0.3009,  ...,  1.0567,  0.0581,  0.7998],\n",
      "         [ 1.1566,  1.3352,  0.8157,  ...,  1.8481, -0.7974,  0.3769],\n",
      "         ...,\n",
      "         [ 0.0195, -0.6253, -0.1937,  ..., -0.0371, -1.3394,  1.1048],\n",
      "         [-1.4537, -1.0194, -0.4173,  ...,  2.5672, -0.5453,  1.6958],\n",
      "         [ 0.1847,  0.0936, -0.0872,  ...,  0.1001,  0.3690,  1.1085]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.9225, -1.0049,  0.5280,  ..., -0.7814,  1.3149,  0.5105],\n",
      "         [ 0.0780, -0.2692, -0.8597,  ...,  1.4842,  0.8182,  0.5812],\n",
      "         [-0.0953,  0.6618,  1.6775,  ..., -1.1406,  0.9152,  1.3962],\n",
      "         ...,\n",
      "         [-2.1495, -1.2366,  1.1979,  ...,  1.3424,  0.0356,  2.2546],\n",
      "         [-1.2604, -0.7092,  0.2089,  ...,  1.1628, -0.5514, -1.2665],\n",
      "         [-1.1928,  0.7524,  0.0993,  ...,  0.3344, -0.7030, -0.3725]],\n",
      "\n",
      "        [[ 0.4109, -1.6922, -0.7335,  ..., -0.6796, -0.1366,  1.8411],\n",
      "         [-0.9323, -0.4430, -1.8910,  ..., -0.1617, -1.2981,  0.1000],\n",
      "         [-0.7917,  0.3008, -0.0431,  ...,  0.2142,  0.1115, -0.5389],\n",
      "         ...,\n",
      "         [-1.5297, -0.8350,  0.1403,  ..., -1.6493, -0.0846, -0.1320],\n",
      "         [ 0.0519, -0.2750,  0.2974,  ..., -0.2975,  0.4899, -1.2170],\n",
      "         [-0.4628,  1.0428, -1.0698,  ..., -0.0446,  0.3562, -0.4886]],\n",
      "\n",
      "        [[ 1.6756, -1.2955,  0.8643,  ..., -2.4229,  0.3482, -0.1859],\n",
      "         [-0.2686,  0.6755, -0.7333,  ..., -0.1982,  0.5164, -0.9123],\n",
      "         [-0.4516, -0.1561,  0.1131,  ...,  0.0863, -0.2396,  0.2016],\n",
      "         ...,\n",
      "         [-1.9327,  0.3595, -0.2242,  ..., -1.3398,  0.0427,  1.0582],\n",
      "         [-1.2419,  0.0063,  0.5507,  ..., -0.9542, -0.3986,  1.3616],\n",
      "         [-0.6057, -0.4603, -0.9190,  ...,  1.1446,  0.6753, -0.0486]]],\n",
      "       grad_fn=<AddBackward0>),\n",
      "output.shape--->torch.Size([128, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, max_len, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = TransformerEmbedding(vocab_size, d_model, max_len)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
    "        self.norm = LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "decoder = Decoder(vocab_size=10000, d_model=d_model, n_layers=6, n_heads=n_heads, max_len=5000)\n",
    "output = decoder(torch.randint(0, 10000, (128, 64)), torch.randn(128, 64, d_model))\n",
    "print(f\"output--->{output},\\noutput.shape--->{output.shape}\")  # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeee57d2",
   "metadata": {},
   "source": [
    "#### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37b683de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output--->tensor([[[ 0.0704, -0.7916,  0.0345,  ...,  0.9713,  0.5251, -0.2577],\n",
      "         [ 0.4158,  0.3712, -0.6179,  ...,  0.2595, -0.3644,  0.4199],\n",
      "         [ 0.2551, -0.7157, -0.1395,  ...,  0.3235, -0.1678, -0.2280],\n",
      "         ...,\n",
      "         [ 1.0497, -1.1774,  0.8196,  ..., -0.1789, -0.4988, -0.0934],\n",
      "         [-0.0275, -0.8394,  0.3046,  ...,  0.3125, -0.4127, -1.0884],\n",
      "         [ 0.8755, -0.2182, -0.3787,  ...,  0.8447, -0.1265, -0.6728]],\n",
      "\n",
      "        [[-0.0675,  0.5254,  0.4661,  ...,  1.0474, -0.7141,  0.2792],\n",
      "         [-0.1281, -0.6991, -0.2546,  ...,  0.2783,  0.1304,  0.6848],\n",
      "         [ 0.5277, -0.5158, -0.2010,  ...,  0.3533, -0.2300, -0.1879],\n",
      "         ...,\n",
      "         [ 0.3603,  0.0540, -0.0661,  ...,  0.3685, -0.1012, -0.1499],\n",
      "         [ 0.5997, -0.2729,  0.6627,  ...,  0.0234, -0.4179, -0.3762],\n",
      "         [ 0.1777, -0.1046, -0.2325,  ..., -0.1315, -0.9080, -0.1761]],\n",
      "\n",
      "        [[ 0.5476, -1.6461,  0.6430,  ...,  0.7594, -1.1301,  0.1436],\n",
      "         [ 0.8264, -0.7490,  0.9204,  ...,  0.2353,  0.0737, -0.1314],\n",
      "         [ 0.3548, -0.9813,  0.0350,  ..., -0.1163, -0.5525, -0.5373],\n",
      "         ...,\n",
      "         [ 0.4604, -1.5278, -0.7293,  ..., -0.4187, -0.2438,  0.2750],\n",
      "         [ 1.0291, -0.7039, -0.3496,  ...,  0.7257,  0.1552,  0.1734],\n",
      "         [ 0.1226, -0.6317,  0.4367,  ..., -0.1168,  0.2560, -0.1983]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1140, -0.3908,  0.2465,  ...,  0.1734, -0.1699,  0.1278],\n",
      "         [-0.3590, -0.8960,  0.8465,  ..., -0.4450,  0.0152, -0.9034],\n",
      "         [ 0.1122, -0.0198,  0.4046,  ..., -0.9087,  0.3395, -0.0938],\n",
      "         ...,\n",
      "         [-0.4055, -0.8170,  0.3061,  ..., -0.5398, -0.1399, -0.0074],\n",
      "         [-0.3883, -0.2605, -0.5834,  ..., -0.5045, -0.4749,  0.3292],\n",
      "         [ 0.1157, -0.6784, -0.2851,  ...,  0.4727,  0.0175, -0.2120]],\n",
      "\n",
      "        [[-0.0451, -1.0884,  1.1548,  ..., -1.0112,  0.4398, -0.3172],\n",
      "         [ 0.2189, -0.0027,  0.8043,  ..., -0.4789,  0.5481, -0.0573],\n",
      "         [ 0.6060, -0.5668, -0.2373,  ...,  0.4877,  0.0268, -0.3930],\n",
      "         ...,\n",
      "         [ 0.2432,  0.5224, -0.4228,  ..., -0.5299,  0.8535,  0.3420],\n",
      "         [-0.0932, -0.0856, -0.7005,  ...,  0.5098,  0.3766, -0.1014],\n",
      "         [ 0.1597, -0.6554,  0.9953,  ...,  0.5717,  0.1717, -0.2777]],\n",
      "\n",
      "        [[-0.4423, -0.5599, -0.5502,  ...,  0.0599, -0.2007,  0.6446],\n",
      "         [ 0.3358, -0.3688,  0.5421,  ..., -0.1212, -0.7835,  0.2891],\n",
      "         [ 0.3212, -0.7220,  0.5865,  ...,  0.1206, -0.6339,  1.2842],\n",
      "         ...,\n",
      "         [-0.1093, -1.4653, -0.2538,  ..., -0.0466,  0.1598,  0.8951],\n",
      "         [-0.2113, -0.8537,  0.0969,  ..., -0.2014, -0.1444,  0.6334],\n",
      "         [ 0.7666,  0.0288, -0.4241,  ..., -0.0885, -0.3700, -0.5044]]],\n",
      "       grad_fn=<ViewBackward0>),\n",
      "output.shape--->torch.Size([128, 64, 10000])\n"
     ]
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,src_pad_idx, tgt_pad_idx, vocab_size, d_model, n_layers, n_heads, max_len, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size, d_model, n_layers, n_heads, max_len, d_ff, dropout)\n",
    "        self.decoder = Decoder(vocab_size, d_model, n_layers, n_heads, max_len, d_ff, dropout)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.tgt_pad_idx = tgt_pad_idx\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def make_pad_mask(self, q, k, pad_idx_q, pad_idx_k):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "        q = q.ne(pad_idx_q).unsqueeze(1).unsqueeze(3)\n",
    "        q = q.repeat(1,1,1,len_k)\n",
    "\n",
    "        k = k.ne(pad_idx_k).unsqueeze(1).unsqueeze(2)\n",
    "        k = k.repeat(1,1,len_q,1)\n",
    "\n",
    "        mask = q & k\n",
    "        return mask\n",
    "\n",
    "\n",
    "\n",
    "    def mask_casual_mask(self, q, k):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "        mask = torch.tril(torch.ones(len_q, len_k, dtype=torch.bool))\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = self.make_pad_mask(src, src, self.src_pad_idx, self.src_pad_idx)\n",
    "        tgt_mask = self.make_pad_mask(tgt, tgt, self.tgt_pad_idx, self.tgt_pad_idx) & self.mask_casual_mask(tgt, tgt)\n",
    "        src_trg_mask = self.make_pad_mask(src, tgt, self.src_pad_idx, self.tgt_pad_idx)\n",
    "\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
    "        output = self.output_layer(dec_output)\n",
    "        return output\n",
    "    \n",
    "transformer = Transformer(src_pad_idx=1, tgt_pad_idx=1, vocab_size=10000, d_model=d_model, n_layers=6, n_heads=n_heads, max_len=5000)\n",
    "src = torch.randint(0, 10000, (128, 64))\n",
    "tgt = torch.randint(0, 10000, (128, 64))\n",
    "output = transformer(src, tgt)\n",
    "print(f\"output--->{output},\\noutput.shape--->{output.shape}\")  # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
